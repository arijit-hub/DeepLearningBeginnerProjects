{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac424a6",
   "metadata": {},
   "source": [
    "## Understanding Self-Attention with Numpy ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce29eef",
   "metadata": {},
   "source": [
    "In this notebook we are going through the understanding of simple self-attention mechanism. We are going to use <b>numpy</b> only and try to get the basic understanding of the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7e9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary packages ##\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ac7da",
   "metadata": {},
   "source": [
    "Before diving into the code base, let take a moment and summarize what needs to be done and how to use self-attention.\n",
    "\n",
    "- [ ] Here, we will use self-attention on sequences, i.e. on an English sentence - \"Self Attention is good\". But, our self-attention model doesn't understand words. So we need to convert each word into a number. The easiest way of doing this is through the creation of one-hot encoded vectors for each word. Since, there are in total 4 words in our sentence, our one hot vector for each word will have dimension 4. So, lets prepare that: self_in_number = [1 , 0 , 0 , 0] ,  attention_in_number = [0 , 1 , 0 , 0] , is_in_number = [0 , 0 , 1 , 0] , and good_in_number = [0, 0 , 0 , 1].\n",
    "- [ ] These vectors will be our inputs for self-attention.\n",
    "- [ ] For self attention we need to figure out the weights of each word on finding the output at the current time step. We calculate the weights by multiplying the input of the current timestep with all the inputs and getting the respective weights. We put this weight through a softmax function to crush overwhelming values. Then we multiply each input with their corresponding weights and add all of these values up to finally get the output of the current step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aacc12",
   "metadata": {},
   "source": [
    "So, lets first make our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0850aaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs are \n",
      "x1 : \n",
      "[1 0 0 0]\n",
      "\n",
      "x2 : \n",
      "[0 1 0 0]\n",
      "\n",
      "x3 : \n",
      "[0 0 1 0]\n",
      "\n",
      "x4 : \n",
      "[0 0 0 1]\n",
      "Combined inputs \n",
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "## Making inputs ##\n",
    "\n",
    "x1 = np.array([1 , 0 , 0 , 0])\n",
    "x2 = np.array([0, 1 , 0 , 0])\n",
    "x3 = np.array([0 , 0 , 1 , 0])\n",
    "x4 = np.array([0 , 0 , 0 , 1])\n",
    "\n",
    "print(f'Inputs are \\nx1 : \\n{x1}\\n\\nx2 : \\n{x2}\\n\\nx3 : \\n{x3}\\n\\nx4 : \\n{x4}')\n",
    "\n",
    "inputs = np.array([x1 , x2 , x3 , x4])\n",
    "print(f'Combined inputs \\n{inputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc387db9",
   "metadata": {},
   "source": [
    "What next? \n",
    "\n",
    "Now we must create our outputs.\n",
    "\n",
    "Since we have 4 inputs we are expected to have 4 outputs.\n",
    "\n",
    "We will initialize our outputs as zeros and will fill them with values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ab7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialiazed outputs : \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## Initializing our output values with zeros ##\n",
    "\n",
    "outputs = np.zeros((4 , 4))\n",
    "\n",
    "print(f'Initialiazed outputs : \\n{outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d7bd08",
   "metadata": {},
   "source": [
    "But, how do we fill the values of our outputs? \n",
    "\n",
    "We calculate the output at step t by a weighted sum of all the inputs with their respective weights. The respective weights are calculated by a simple product of a input vector with the input vector at time step t.\n",
    "\n",
    "So, let do it :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20d10fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for word 0 is [0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      "Weight for word 1 is [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      "Weight for word 2 is [0.1748777  0.1748777  0.47536689 0.1748777 ]\n",
      "Weight for word 3 is [0.1748777  0.1748777  0.1748777  0.47536689]\n",
      "Outputs is [[0.47536689 0.1748777  0.1748777  0.1748777 ]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.1748777  0.1748777  0.47536689 0.1748777 ]\n",
      " [0.1748777  0.1748777  0.1748777  0.47536689]]\n"
     ]
    }
   ],
   "source": [
    "## Filling the values of output ##\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    weight = np.zeros(outputs.shape[0])\n",
    "    for j in range(len(inputs)):\n",
    "        weight[j] = np.dot(inputs[i] ,  inputs[j])\n",
    "    weight = np.exp(weight) / np.sum(np.exp(weight))\n",
    "    print(f'Weight for word {i} is {weight}')\n",
    "    \n",
    "    for j in range(len(inputs)):\n",
    "        outputs[i] += inputs[j] * weight[j]\n",
    "    \n",
    "print(f'Outputs is {outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5057770",
   "metadata": {},
   "source": [
    "Woohoo as expected, the output resembles the input since there is no connection between words. If the one-hot representation is changed to an embedding representation also, you will observe the differences.\n",
    "\n",
    "But before we move on to that, lets just wait a second and discuss this question: \"Is the previous solution efficient?\". The answer is ofcourse not. We can easily remove the loops with the help of matrices. So lets do it.\n",
    "\n",
    "Also this time we won't be using one hot encoded values but will be using random values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a54c4",
   "metadata": {},
   "source": [
    "#### We hate loops! Let the fun begin with matrices ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14dcad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is \n",
      "[[ 0.05056171  0.49995133 -0.99590893  0.69359851]\n",
      " [-0.41830152 -1.58457724 -0.64770677  0.59857517]\n",
      " [ 0.33225003 -1.14747663  0.61866969 -0.08798693]\n",
      " [ 0.4250724   0.33225315 -1.15681626  0.35099715]\n",
      " [-0.60688728  1.54697933  0.72334161  0.04613557]\n",
      " [-0.98299165  0.05443274  0.15989294 -1.20894816]]\n",
      "Input shape is (6, 4)\n",
      "Weights is \n",
      "[[0.41676423 0.03317058 0.02581661 0.36795706 0.03431829 0.02264204]\n",
      " [0.09500843 0.82755424 0.30216376 0.09750615 0.0023233  0.03800751]\n",
      " [0.0216077  0.08829633 0.54601934 0.02816939 0.00701479 0.05226323]\n",
      " [0.36149091 0.03344438 0.03306499 0.435864   0.01848149 0.02289683]\n",
      " [0.07837035 0.00185235 0.01913955 0.04295988 0.86969345 0.13172454]\n",
      " [0.02675839 0.01568212 0.07379574 0.02754352 0.06816868 0.73246585]]\n",
      "Weights shape is (6, 6)\n",
      "Checking if softmax is working : [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "## Setting our input ##\n",
    "np.random.seed(4)\n",
    "inp = np.random.randn(6 , 4) # each input is of dimension 4 #\n",
    "print(f'Input is \\n{inp}')\n",
    "print(f'Input shape is {inp.shape}')\n",
    "\n",
    "## Setting our weights ##\n",
    "# our previous weights were nothing but the dot product of current step input and all other inputs ## \n",
    "\n",
    "weights = np.dot(inp , inp.T) # shape is (6 , 6) #\n",
    "\n",
    "# We must softmax the weights #\n",
    "weights = np.exp(weights)\n",
    "weights = weights / np.sum(weights , axis = 0)\n",
    "print(f'Weights is \\n{weights}')\n",
    "print(f'Weights shape is {weights.shape}')\n",
    "print(f'Checking if softmax is working : {np.sum(weights , axis = 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034379a",
   "metadata": {},
   "source": [
    "So lets get our output !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d85d643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41676423, 0.03317058, 0.02581661, 0.36795706, 0.03431829,\n",
       "        0.02264204],\n",
       "       [0.41676423, 0.03317058, 0.02581661, 0.36795706, 0.03431829,\n",
       "        0.02264204],\n",
       "       [0.41676423, 0.03317058, 0.02581661, 0.36795706, 0.03431829,\n",
       "        0.02264204],\n",
       "       [0.41676423, 0.03317058, 0.02581661, 0.36795706, 0.03431829,\n",
       "        0.02264204]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = weights[:][0].reshape(1 , -1)\n",
    "np.repeat(a , 4 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ee168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (6, 4)\n",
      "current weight shape : (6, 1)\n",
      "current weight shape : (6, 1)\n",
      "current weight shape : (6, 1)\n",
      "current weight shape : (6, 1)\n",
      "current weight shape : (6, 1)\n",
      "current weight shape : (6, 1)\n",
      "Output is \n",
      "[[ 0.1290987   0.30275356 -0.81778664  0.41001273]\n",
      " [-0.23829336 -1.5724902  -0.54873169  0.52304713]\n",
      " [ 0.1019155  -0.73259804  0.23993999 -0.03317603]\n",
      " [ 0.16682378  0.26444536 -0.84840296  0.39399784]\n",
      " [-0.62948215  1.38112546  0.53304216 -0.05026326]\n",
      " [-0.73035879  0.05832655  0.14341142 -0.8512471 ]]\n"
     ]
    }
   ],
   "source": [
    "## Getting output ##\n",
    "\n",
    "outputs = np.zeros(inp.shape)\n",
    "print(f'output shape {outputs.shape}')\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    current_weight = weights[:][i].reshape(-1 , 1)\n",
    "    print(f'current weight shape : {current_weight.shape}')\n",
    "    outputs[i] = np.sum(np.multiply(inp , current_weight) , axis = 0)\n",
    "    \n",
    "print(f'Output is \\n{outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1e68a",
   "metadata": {},
   "source": [
    "And done :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee760d",
   "metadata": {},
   "source": [
    "This was concise and I hope this clears a lot of doubt regarding self-attention.\n",
    "We are going to build up on this concept and work on with our Transformers models using Pytorch soon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
