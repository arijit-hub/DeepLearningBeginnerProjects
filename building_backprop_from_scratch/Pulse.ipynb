{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a9b1a0",
   "metadata": {},
   "source": [
    "### Building Pulse: A small autograd based neural network library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fe02d",
   "metadata": {},
   "source": [
    "*This work is highly motivated by the awesome [video](https://youtu.be/VMj-3S1tku0) made by Andrej Karpathy.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e7346",
   "metadata": {},
   "source": [
    "In this project I will try to break down each piece of the puzzle of building a neural network and explain it in each step of the way. I would be as naive as possible and will always put understanding before performance.\n",
    "\n",
    "I hope you will also enjoy this as much as I enjoyed building this from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c202f",
   "metadata": {},
   "source": [
    "First up is always importing the necessary libraries. Since, we are going to be doing very basic thing, I will just import math, and that's it. (I am not kidding :D )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary libraries ##\n",
    "\n",
    "\n",
    "import math\n",
    "import graphviz\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f141e3a",
   "metadata": {},
   "source": [
    "The heart of our library is the **Pulse** module. It holds the basic operations. I intend to add more and more and I go on. \n",
    "\n",
    "So, let's get give your heart's pulse (pun!) a race and start building ourselves **Pulse**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460187f0",
   "metadata": {},
   "source": [
    "So, what are we going to do? What are the steps we need to take? Let's write these up in plain English, and I will setup checkboxes such that I can tick them as done once implemented.\n",
    "\n",
    "- Creation of a class called Pulse. \n",
    "\n",
    "- We need to have a constructor of this class Pulse, which holds the data that is given by the user. Since, our Pulse only supports numerical value we must do a sanity check for the types. Our constructor must also hold a few other things: \n",
    "    - [x] grad : Gradient of the current pulse with respect to the last node. We initialize it as 0.0.\n",
    "    - [x] children : The nodes from which the current Pulse node is created. It is initialized with an empty list.\n",
    "    - [x] operation : The operation symbol that we are doing with the current node. It is a string. It is initialized with an empty string.\n",
    "    - [x] name : Name of the node. Initialized with string.\n",
    "    \n",
    "- Adding operations. Each neuron in a neural network is manipulated via certain operations.\n",
    "\n",
    "    - [x] First we implement the addition operation. This can be done via the python in-built method of ` __add__(self,other)`. This works when a number is added to any Pulse object. Eg. Pulse(4) + Pulse(5) works. We alse need to add the backward gradient of the nodes from which the resulting node is created. For addition, gradient is routed and hence, we just need to accumulate the gradient of the result node on the previous nodes. Apart from this, if we give an operation like Pulse(4) + 5 won't work since 5 is not a Pulse object. Hence if there is any\n",
    "    - [x] The `__add__()` method doesn't work when there is 5 + Pulse(4) instead of Pulse(4) + 5. So we are needed to implement the `__radd__()` method and the same thing repeats here.\n",
    "    - [x] Similar to the addition we add the multiplication operation. We do this via the `__mul__(self,other)` method of python classes. We also incorporate `__rmul__(self,other)` method to adjust for left multiplication. The backward pass of this neuron also calculates the gradient of the previous nodes. The gradient of previous nodes is very trivial and just takes into account the other nodes' value.\n",
    "    - [x] The `__neg__` method does negation operation and the `__pow__` and `__rpow__` method does the power operation and are implemented accordingly.\n",
    "    - [x] Similarly the implementation for division is done via `__truediv__` and `__rtruediv__` magic methods.\n",
    "    - [x] We also implement the `tanh` method to incorporate tanh function.\n",
    "    \n",
    "- Adding `backward()` method, which topologically sorts all the nodes starting from the current calling node and then calculates the gradient of each of the previous nodes by first initiating the gradient of the current node to 1 and then calling the `._backward()` for each of the nodes one at a time. This calculates the gradient of each of the node with respect to the called node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bf234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pulse module ##\n",
    "\n",
    "class Pulse:\n",
    "    \"\"\"The core module of our neural network library.\"\"\"\n",
    "    def __init__(self, data, name = '' , children = [] , operation = ''):\n",
    "        \"\"\"Constructor.\"\"\"\n",
    "        \n",
    "        assert type(data) == float or type(data) == int , \"The data must be an int or a float!\"\n",
    "        \n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self.name = name\n",
    "        self._children = []\n",
    "        self._children.extend(children)\n",
    "        self._operation = operation\n",
    "        self._backward = lambda : None\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Does backward gradient calculation of current node with respect to\n",
    "        all the previous nodes\"\"\"\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "    \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation of Pulse.\"\"\"\n",
    "        \n",
    "        return f\"Pulse({self.data} , name = \\\"{self.name}\\\")\"\n",
    "    \n",
    "    def __add__(self , other):\n",
    "        \"\"\"Addition operation of Pulse.\"\"\"\n",
    "        \n",
    "        if not isinstance(other, Pulse):\n",
    "            other = Pulse(other)\n",
    "            \n",
    "        result = Pulse(self.data + other.data, children = [self , other], operation='+')\n",
    "        \n",
    "        def _backward_():\n",
    "            other.grad += 1.0 * result.grad\n",
    "            self.grad += 1.0 * result.grad\n",
    "        \n",
    "        result._backward = _backward_ \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __radd__(self ,other):\n",
    "        \"\"\"Reverse Addition operation of Pulse.\"\"\"\n",
    "        \n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self , other):\n",
    "        \"\"\"Multiplication operation of Pulse.\"\"\"\n",
    "        \n",
    "        if not isinstance(other, Pulse):\n",
    "            other = Pulse(other)\n",
    "            \n",
    "        result = Pulse(self.data * other.data, children = [self , other], operation='*')\n",
    "        \n",
    "        def _backward_():\n",
    "            other.grad += self.data * result.grad\n",
    "            self.grad += other.data * result.grad\n",
    "        \n",
    "        result._backward = _backward_ \n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __rmul__(self ,other):\n",
    "        \"\"\"Reverse Multiplication operation of Pulse.\"\"\"\n",
    "        \n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self , other):\n",
    "        \"\"\"Implements the power operation.\"\"\"\n",
    "        #print(self)\n",
    "        other_grad = True\n",
    "        \n",
    "        if not isinstance(other, Pulse):\n",
    "            other_grad = False\n",
    "            other = Pulse(data = other, name = f'{other}')\n",
    "        \n",
    "        result = Pulse(self.data ** other.data , children = [self,other] , operation = '**')\n",
    "        \n",
    "        def _backward_():\n",
    "            self.grad += (other.data) * ((self.data) ** (other.data - 1)) * (result.grad) \n",
    "            if other_grad : \n",
    "                other.grad += (self.data ** other.data) * math.log(self.data)\n",
    "            \n",
    "        result._backward = _backward_\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __rpow__(self , other):\n",
    "        \"\"\"Reverse power.\"\"\"\n",
    "        other_grad = True\n",
    "        if not isinstance(other , Pulse):\n",
    "            other_grad = False\n",
    "            other = Pulse(data = other , name = f'{other}')\n",
    "            \n",
    "        result = Pulse(other.data ** self.data , children = [self,other] , operation = '**')\n",
    "        \n",
    "        def _backward_():\n",
    "            self.grad = (other.data ** self.data) * math.log(other.data)\n",
    "            if other_grad:\n",
    "                other.grad += (other.data) * ((self.data) ** (other.data - 1)) * (result.grad)\n",
    "            \n",
    "        result._backward = _backward_\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"Negation of Pulse.\"\"\"\n",
    "        \n",
    "        return self * -1\n",
    "    \n",
    "    def __truediv__(self , other):\n",
    "        \"\"\"Division operation for pulse.\"\"\"\n",
    "        \n",
    "        return self * (other ** -1)\n",
    "    \n",
    "    def __rtruediv__(self , other):\n",
    "        \"\"\"Division operation for pulse.\"\"\"\n",
    "        \n",
    "        return other * self**-1\n",
    "    \n",
    "    def __sub__(self , other):\n",
    "        \"\"\"Substraction operation for Pulse.\"\"\"\n",
    "        return self + (-other)\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"Tanh function.\"\"\"\n",
    "        if not isinstance(self , Pulse):\n",
    "            self = Pulse(data = self , name = f'{self}')\n",
    "        \n",
    "        result = (math.exp(self.data) - math.exp(-self.data)) / (math.exp(self.data) + math.exp(-self.data)) \n",
    "        \n",
    "        result = Pulse(result, children=[self], operation='tanh')\n",
    "        \n",
    "        def _backward_():\n",
    "            self.grad += (1 - result.data ** 2) * result.grad\n",
    "        \n",
    "        result._backward = _backward_\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730330d",
   "metadata": {},
   "source": [
    "We all love a Visualization toolkit. But, graph visualization can be a bit tricky. In our project we are going to use ```graphviz``` to visualize our graph. You can have a look at their [documentation](https://graphviz.readthedocs.io/en/stable/manual.html) to learn more.\n",
    "\n",
    "As a next step we are going to implement our very own utility function for visualization. I am going to explain each step so we are on track to understanding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c51d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##The following code is entirely copied from the video stated above ##\n",
    "\n",
    "def trace(root):\n",
    "    \"\"\"Builds a set of all nodes and edges in a graph\"\"\"\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._children:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def visualize(root):\n",
    "    \"\"\"Visualizes a directed graph.\"\"\"\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.name, n.data, n.grad), shape='record')\n",
    "        if n._operation:\n",
    "          # if this value is a result of some operation, create an op node for it\n",
    "          dot.node(name = uid + n._operation, label = n._operation)\n",
    "          # and connect this node to it\n",
    "          dot.edge(uid + n._operation, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._operation)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1c8f4",
   "metadata": {},
   "source": [
    "Now its time to build up a small **Multilayer Perceptron** using `Pulse`. But before we make our hands dirty and build up a MLP we need to construct its minimal unit of layers which consists of Neuron. We can think of this neuron as a Pulse object and n number of neurons make a layer. So lets create that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4246de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neuron implementation\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Creates a single neuron.\"\"\"\n",
    "    def __init__(self , num_prev_neurons):\n",
    "        self.weights = []\n",
    "        for idx in range(num_prev_neurons):   \n",
    "            self.weights.append(Pulse(data = random.uniform(-1 , 1)))\n",
    "        \n",
    "        self.bias = Pulse(data = random.uniform(-1 , 1))\n",
    "    \n",
    "    def __call__(self , x):\n",
    "        return sum((w_i * x_i for w_i , x_i in zip(self.weights , x)) , self.bias).tanh()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "\n",
    "## Layer Implementation\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"Creates a layer of neurons.\"\"\"\n",
    "    def __init__(self , num_in_neurons , num_out_neurons):\n",
    "        self.neurons = [Neuron(num_in_neurons) for _ in range(num_out_neurons)]\n",
    "    \n",
    "    def __call__(self , x):\n",
    "        out =  [neuron(x) for neuron in self.neurons]\n",
    "        if len(out) == 1:\n",
    "            return out[0]\n",
    "        else:\n",
    "            return out\n",
    "    def parameters(self):\n",
    "        return [each_parameter for neuron in self.neurons for each_parameter in neuron.parameters()]\n",
    "    \n",
    "class MLP:\n",
    "    \"\"\"Creates a multi-layer perceptron.\"\"\"\n",
    "    def __init__(self , in_size , out_sizes):\n",
    "        layer_sizes = [in_size] + out_sizes\n",
    "        self.layers = [Layer(inp_size , out_size) for idx , (inp_size , out_size) in enumerate(zip(\n",
    "            layer_sizes , layer_sizes[1:]\n",
    "        ))]\n",
    "    \n",
    "    def __call__(self , x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [each_parameter for layer in self.layers for each_parameter in layer.parameters()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
